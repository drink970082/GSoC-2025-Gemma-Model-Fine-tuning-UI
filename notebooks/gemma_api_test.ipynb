{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "from config.training_config import TENSORBOARD_LOGDIR\n",
    "\n",
    "\n",
    "class TensorBoardDataManager:\n",
    "    \"\"\"Manages TensorBoard event data loading and caching.\"\"\"\n",
    "\n",
    "    def __init__(self, log_dir: str = TENSORBOARD_LOGDIR, ignore_timing: bool = False):\n",
    "        self.log_dir = log_dir\n",
    "        self._event_data = {}\n",
    "        self._manager_tracking_time = time.time()\n",
    "        self.ignore_timing = ignore_timing  # Allow reading all files regardless of timing\n",
    "\n",
    "    def reset_training_time(self):\n",
    "        \"\"\"Reset the training start time to current time.\"\"\"\n",
    "        self._manager_tracking_time = time.time()\n",
    "        self.clear_cache()\n",
    "\n",
    "    def list_all_event_files(self) -> list[tuple[str, float, float]]:\n",
    "        \"\"\"List all event files with their creation and modification times.\"\"\"\n",
    "        event_files = []\n",
    "        for root, _, files in os.walk(self.log_dir):\n",
    "            for file in files:\n",
    "                if file.startswith(\"events.out.tfevents.\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    created_time = os.path.getctime(file_path)\n",
    "                    modified_time = os.path.getmtime(file_path)\n",
    "                    event_files.append((file_path, created_time, modified_time))\n",
    "        \n",
    "        # Sort by modification time (newest first)\n",
    "        event_files.sort(key=lambda x: x[2], reverse=True)\n",
    "        return event_files\n",
    "\n",
    "    def examine_event_file(self, file_path: str) -> dict:\n",
    "        \"\"\"Examine a specific event file and return its contents.\"\"\"\n",
    "        try:\n",
    "            event_acc = EventAccumulator(\n",
    "                file_path,\n",
    "                size_guidance={\"scalars\": 0, \"tensors\": 0},\n",
    "            )\n",
    "            event_acc.Reload()\n",
    "            \n",
    "            tags = event_acc.Tags()\n",
    "            result = {\n",
    "                \"file_path\": file_path,\n",
    "                \"tags\": tags,\n",
    "                \"tensor_data\": {}\n",
    "            }\n",
    "            \n",
    "            # Process tensor metrics\n",
    "            for tag in tags.get(\"tensors\", []):\n",
    "                try:\n",
    "                    events = event_acc.Tensors(tag)\n",
    "                    result[\"tensor_data\"][tag] = {\n",
    "                        \"num_events\": len(events),\n",
    "                        \"events\": []\n",
    "                    }\n",
    "                    \n",
    "                    for i, event in enumerate(events[:5]):  # Show first 5 events\n",
    "                        value = self._parse_tensor_value(event.tensor_proto)\n",
    "                        result[\"tensor_data\"][tag][\"events\"].append({\n",
    "                            \"index\": i,\n",
    "                            \"step\": event.step,\n",
    "                            \"wall_time\": event.wall_time,\n",
    "                            \"value\": value\n",
    "                        })\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    result[\"tensor_data\"][tag] = {\"error\": str(e)}\n",
    "            \n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            return {\"file_path\": file_path, \"error\": str(e)}\n",
    "\n",
    "    def _find_latest_event_file(self) -> str:\n",
    "        \"\"\"Find the latest event file in the log directory.\"\"\"\n",
    "        event_files = self.list_all_event_files()\n",
    "        \n",
    "        if not event_files:\n",
    "            return None\n",
    "\n",
    "        if self.ignore_timing:\n",
    "            # Return the most recently modified file\n",
    "            return event_files[0][0]\n",
    "        else:\n",
    "            # Only consider files created after manager initialization\n",
    "            relevant_files = [\n",
    "                file_path for file_path, created_time, _ in event_files\n",
    "                if created_time >= self._manager_tracking_time\n",
    "            ]\n",
    "            return relevant_files[0] if relevant_files else None\n",
    "\n",
    "    def _load_event_data(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Load and parse event data from TensorBoard logs.\"\"\"\n",
    "        data = {}\n",
    "\n",
    "        try:\n",
    "            # Find latest event file\n",
    "            latest_event_file = self._find_latest_event_file()\n",
    "            if not latest_event_file:\n",
    "                return data\n",
    "\n",
    "            # Always create new EventAccumulator and reload to get latest data\n",
    "            event_acc = EventAccumulator(\n",
    "                latest_event_file,\n",
    "                size_guidance={\"scalars\": 0, \"tensors\": 0},\n",
    "            )\n",
    "            event_acc.Reload()\n",
    "\n",
    "            # Get available tags\n",
    "            tags = event_acc.Tags()\n",
    "\n",
    "            # Process tensor metrics\n",
    "            for tag in tags.get(\"tensors\", []):\n",
    "                try:\n",
    "                    events = event_acc.Tensors(tag)\n",
    "\n",
    "                    # Handle different types of tensor data\n",
    "                    if tag in [\n",
    "                        \"parameters\",\n",
    "                        \"num_params\",\n",
    "                        \"element_spec\",\n",
    "                        \"context_spec\",\n",
    "                    ]:\n",
    "                        # Metadata tensors (single values)\n",
    "                        if events:\n",
    "                            event = events[0]\n",
    "                            value = self._parse_tensor_value(event.tensor_proto)\n",
    "\n",
    "                            data[tag] = pd.DataFrame(\n",
    "                                [\n",
    "                                    {\n",
    "                                        \"wall_time\": event.wall_time,\n",
    "                                        \"step\": event.step,\n",
    "                                        \"value\": value,\n",
    "                                        \"description\": f\"Model metadata: {tag}\",\n",
    "                                    }\n",
    "                                ],\n",
    "                                columns=[\n",
    "                                    \"wall_time\",\n",
    "                                    \"step\",\n",
    "                                    \"value\",\n",
    "                                    \"description\",\n",
    "                                ],\n",
    "                            )\n",
    "\n",
    "                    elif tag.startswith(\"losses/\") or tag.startswith(\n",
    "                        \"perf_stats/\"\n",
    "                    ):\n",
    "                        # Training metrics (time series)\n",
    "                        parsed_events = []\n",
    "                        for e in events:\n",
    "                            val = self._parse_tensor_value(e.tensor_proto)\n",
    "                            parsed_events.append((e.wall_time, e.step, val))\n",
    "\n",
    "                        data[tag] = pd.DataFrame(\n",
    "                            parsed_events,\n",
    "                            columns=[\"wall_time\", \"step\", \"value\"],\n",
    "                        )\n",
    "\n",
    "                    else:\n",
    "                        # Other tensor types\n",
    "                        data[tag] = pd.DataFrame(\n",
    "                            [\n",
    "                                (\n",
    "                                    e.wall_time,\n",
    "                                    e.step,\n",
    "                                    f\"Tensor data: {e.tensor_proto.dtype}\",\n",
    "                                )\n",
    "                                for e in events\n",
    "                            ],\n",
    "                            columns=[\"wall_time\", \"step\", \"value\"],\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        return data\n",
    "\n",
    "    def _parse_tensor_value(self, tensor_proto) -> any:\n",
    "        \"\"\"Parse tensor value based on its type.\"\"\"\n",
    "        if hasattr(tensor_proto, \"float_val\") and tensor_proto.float_val:\n",
    "            return float(tensor_proto.float_val[0])\n",
    "        elif hasattr(tensor_proto, \"int64_val\") and tensor_proto.int64_val:\n",
    "            return int(tensor_proto.int64_val[0])\n",
    "        elif hasattr(tensor_proto, \"string_val\") and tensor_proto.string_val:\n",
    "            return tensor_proto.string_val[0].decode(\"utf-8\")\n",
    "        elif (\n",
    "            hasattr(tensor_proto, \"tensor_content\")\n",
    "            and tensor_proto.tensor_content\n",
    "        ):\n",
    "            # Handle TensorProto format\n",
    "            if tensor_proto.dtype == 1:  # DT_FLOAT\n",
    "                tensor_data = np.frombuffer(\n",
    "                    tensor_proto.tensor_content, dtype=np.float32\n",
    "                )\n",
    "                return float(tensor_data[0]) if len(tensor_data) > 0 else 0.0\n",
    "            elif tensor_proto.dtype == 3:  # DT_INT32\n",
    "                tensor_data = np.frombuffer(\n",
    "                    tensor_proto.tensor_content, dtype=np.int32\n",
    "                )\n",
    "                return int(tensor_data[0]) if len(tensor_data) > 0 else 0\n",
    "            elif tensor_proto.dtype == 9:  # DT_INT64\n",
    "                tensor_data = np.frombuffer(\n",
    "                    tensor_proto.tensor_content, dtype=np.int64\n",
    "                )\n",
    "                return int(tensor_data[0]) if len(tensor_data) > 0 else 0\n",
    "            elif tensor_proto.dtype == 7:  # DT_STRING\n",
    "                try:\n",
    "                    return tensor_proto.tensor_content.decode(\"utf-8\")\n",
    "                except:\n",
    "                    return f\"String tensor: {len(tensor_proto.tensor_content)} bytes\"\n",
    "            else:\n",
    "                return f\"Shape: {list(tensor_proto.tensor_shape.dim)}, Dtype: {tensor_proto.dtype}\"\n",
    "        else:\n",
    "            return f\"Shape: {list(tensor_proto.tensor_shape.dim)}, Dtype: {tensor_proto.dtype}\"\n",
    "\n",
    "    def get_data(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Get current event data (always loads fresh data).\"\"\"\n",
    "        self._event_data = self._load_event_data()\n",
    "        return self._event_data\n",
    "\n",
    "    def get_metadata(self) -> dict[str, any]:\n",
    "        \"\"\"Get metadata tensors only.\"\"\"\n",
    "        data = self.get_data()\n",
    "        return {\n",
    "            k: v.iloc[0][\"value\"] if not v.empty else None\n",
    "            for k, v in data.items()\n",
    "            if k in [\"num_params\", \"parameters\", \"element_spec\", \"context_spec\"]\n",
    "        }\n",
    "\n",
    "    def get_training_metrics(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Get training metrics only.\"\"\"\n",
    "        data = self.get_data()\n",
    "        return {\n",
    "            k: v\n",
    "            for k, v in data.items()\n",
    "            if k.startswith((\"losses/\", \"perf_stats/\"))\n",
    "        }\n",
    "\n",
    "    def get_latest_values(self) -> dict[str, any]:\n",
    "        \"\"\"Get latest values for all training metrics.\"\"\"\n",
    "        training_data = self.get_training_metrics()\n",
    "        latest_values = {}\n",
    "\n",
    "        for metric_name, metric_df in training_data.items():\n",
    "            if not metric_df.empty:\n",
    "                latest_values[metric_name] = metric_df.iloc[-1][\"value\"]\n",
    "\n",
    "        return latest_values\n",
    "\n",
    "    def get_loss_metrics(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Get loss metrics only.\"\"\"\n",
    "        data = self.get_data()\n",
    "        return {k: v for k, v in data.items() if k.startswith(\"losses/\")}\n",
    "\n",
    "    def get_performance_metrics(self) -> dict[str, pd.DataFrame]:\n",
    "        \"\"\"Get performance metrics only.\"\"\"\n",
    "        data = self.get_data()\n",
    "        return {k: v for k, v in data.items() if k.startswith(\"perf_stats/\")}\n",
    "\n",
    "    def get_current_step(self) -> int:\n",
    "        \"\"\"Get current training step.\"\"\"\n",
    "        training_data = self.get_training_metrics()\n",
    "        if (\n",
    "            \"losses/loss\" in training_data\n",
    "            and not training_data[\"losses/loss\"].empty\n",
    "        ):\n",
    "            return training_data[\"losses/loss\"].iloc[-1][\"step\"]\n",
    "        return 0\n",
    "\n",
    "    def get_current_loss(self) -> float:\n",
    "        \"\"\"Get current loss value.\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\"losses/loss\", 0.0)\n",
    "\n",
    "    def get_training_speed(self) -> float:\n",
    "        \"\"\"Get current training speed (steps/sec).\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\"perf_stats/steps_per_sec\", 0.0)\n",
    "\n",
    "    def get_training_time(self) -> float:\n",
    "        \"\"\"Get total training time (hours).\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\"perf_stats/total_training_time_hours\", 0.0)\n",
    "\n",
    "    def get_data_throughput(self) -> float:\n",
    "        \"\"\"Get data throughput (points/sec).\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\"perf_stats/data_points_per_sec_global\", 0.0)\n",
    "\n",
    "    def get_avg_step_time(self) -> float:\n",
    "        \"\"\"Get average step time (seconds).\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\"perf_stats/train/avg_time_sec\", 0.0)\n",
    "\n",
    "    def get_avg_eval_time(self) -> float:\n",
    "        \"\"\"Get average evaluation time (seconds).\"\"\"\n",
    "        latest_values = self.get_latest_values()\n",
    "        return latest_values.get(\n",
    "            \"perf_stats/evals_along_train/avg_time_sec\", 0.0\n",
    "        )\n",
    "\n",
    "    def clear_cache(self):\n",
    "        \"\"\"Clear cached data.\"\"\"\n",
    "        self._event_data = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä KEY PERFORMANCE INDICATORS\n",
      "============================================================\n",
      "\n",
      "üèóÔ∏è MODEL INFORMATION\n",
      "------------------------------\n",
      "Model Parameters: 999,885,952\n",
      "Total Parameters: 999,885,952\n",
      "Total Memory: 1,999,771,904 bytes (1907.1 MB)\n",
      "Layers: 286\n",
      "Batch Size: 4\n",
      "Sequence Length: 200\n",
      "Input Shape: [4, 200]\n",
      "Input Dtype: int64\n",
      "Batch Specs: 3\n",
      "Gradient Specs: 288\n",
      "Total Specs: 2105\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "def parse_parameter_summary(param_text: str) -> dict:\n",
    "    \"\"\"Parse parameter summary text and extract key information.\"\"\"\n",
    "    if not param_text or not isinstance(param_text, str):\n",
    "        return {}\n",
    "    \n",
    "    result = {\n",
    "        \"total_params\": None,\n",
    "        \"total_bytes\": None,\n",
    "        \"layers\": [],\n",
    "        \"parameter_count\": 0\n",
    "    }\n",
    "    \n",
    "    lines = param_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('|--') or line.startswith('==='):\n",
    "            continue\n",
    "            \n",
    "        # Extract total information\n",
    "        if 'Total:' in line:\n",
    "            # Format: \"Total: 999,885,952 -- 1,999,771,904 bytes\"\n",
    "            parts = line.split('Total:')[1].strip()\n",
    "            if '--' in parts:\n",
    "                params_part, bytes_part = parts.split('--')\n",
    "                result[\"total_params\"] = int(params_part.strip().replace(',', ''))\n",
    "                result[\"total_bytes\"] = int(bytes_part.strip().replace(',', '').replace(' bytes', ''))\n",
    "        \n",
    "        # Extract layer information\n",
    "        elif '|' in line and 'layer_' in line:\n",
    "            # Format: \"| layer_9/pre_attention_norm/scale   | (1152,)           | bfloat16 | 1,152       | 7.97      | 7.44    | ()       |\"\n",
    "            parts = [p.strip() for p in line.split('|') if p.strip()]\n",
    "            if len(parts) >= 4:\n",
    "                layer_info = {\n",
    "                    \"name\": parts[0],\n",
    "                    \"shape\": parts[1],\n",
    "                    \"dtype\": parts[2],\n",
    "                    \"params\": int(parts[3].replace(',', '')) if parts[3].replace(',', '').isdigit() else 0\n",
    "                }\n",
    "                result[\"layers\"].append(layer_info)\n",
    "                result[\"parameter_count\"] += layer_info[\"params\"]\n",
    "    \n",
    "    return result\n",
    "\n",
    "def parse_element_spec(spec_text: str) -> dict:\n",
    "    \"\"\"Parse element spec JSON format.\"\"\"\n",
    "    if not spec_text or not isinstance(spec_text, str):\n",
    "        return {}\n",
    "    \n",
    "    try:\n",
    "        # Extract JSON from the text (remove the ```python wrapper)\n",
    "        json_start = spec_text.find('{')\n",
    "        json_end = spec_text.rfind('}') + 1\n",
    "        if json_start != -1 and json_end != -1:\n",
    "            json_str = spec_text[json_start:json_end]\n",
    "            spec_data = json.loads(json_str)\n",
    "            \n",
    "            result = {\n",
    "                \"input_shape\": spec_data.get(\"input\", {}).get(\"shape\", []),\n",
    "                \"input_dtype\": spec_data.get(\"input\", {}).get(\"dtype\", \"\"),\n",
    "                \"loss_mask_shape\": spec_data.get(\"loss_mask\", {}).get(\"shape\", []),\n",
    "                \"target_shape\": spec_data.get(\"target\", {}).get(\"shape\", []),\n",
    "                \"batch_size\": spec_data.get(\"input\", {}).get(\"shape\", [0])[0] if spec_data.get(\"input\", {}).get(\"shape\") else 0,\n",
    "                \"sequence_length\": spec_data.get(\"input\", {}).get(\"shape\", [0, 0])[1] if len(spec_data.get(\"input\", {}).get(\"shape\", [])) > 1 else 0\n",
    "            }\n",
    "            return result\n",
    "    except (json.JSONDecodeError, KeyError, IndexError):\n",
    "        pass\n",
    "    \n",
    "    return {}\n",
    "\n",
    "def parse_context_spec(spec_text: str) -> dict:\n",
    "    \"\"\"Parse context spec table format.\"\"\"\n",
    "    if not spec_text or not isinstance(spec_text, str):\n",
    "        return {}\n",
    "    \n",
    "    result = {\n",
    "        \"batch_specs\": [],\n",
    "        \"grad_specs\": [],\n",
    "        \"total_specs\": 0\n",
    "    }\n",
    "    \n",
    "    lines = spec_text.split('\\n')\n",
    "    \n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith('|--') or line.startswith('===') or 'Path' in line or 'Spec' in line:\n",
    "            continue\n",
    "            \n",
    "        # Format: \"| `batch.input` | `i32[4 200]` |\"\n",
    "        if '|' in line and '`' in line:\n",
    "            parts = [p.strip() for p in line.split('|') if p.strip()]\n",
    "            if len(parts) >= 2:\n",
    "                path = parts[0].strip('`')\n",
    "                spec = parts[1].strip('`')\n",
    "                \n",
    "                spec_info = {\n",
    "                    \"path\": path,\n",
    "                    \"spec\": spec\n",
    "                }\n",
    "                \n",
    "                if path.startswith('batch.'):\n",
    "                    result[\"batch_specs\"].append(spec_info)\n",
    "                elif path.startswith('grads.'):\n",
    "                    result[\"grad_specs\"].append(spec_info)\n",
    "                \n",
    "                result[\"total_specs\"] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "def print_formatted_kpi():\n",
    "    \"\"\"Print KPI values with parsed metadata information.\"\"\"\n",
    "    manager = TensorBoardDataManager(ignore_timing=True)\n",
    "    \n",
    "    metadata = manager.get_metadata()\n",
    "    latest_values = manager.get_latest_values()\n",
    "    \n",
    "    if not metadata and not latest_values:\n",
    "        print(\"‚è≥ Waiting for training data...\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä KEY PERFORMANCE INDICATORS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Display metadata with parsing\n",
    "    if metadata:\n",
    "        print(\"\\nüèóÔ∏è MODEL INFORMATION\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        if \"num_params\" in metadata and metadata[\"num_params\"]:\n",
    "            print(f\"Model Parameters: {metadata['num_params']:,}\")\n",
    "        \n",
    "        if \"parameters\" in metadata and metadata[\"parameters\"]:\n",
    "            param_summary = parse_parameter_summary(str(metadata[\"parameters\"]))\n",
    "            if param_summary.get(\"total_params\"):\n",
    "                print(f\"Total Parameters: {param_summary['total_params']:,}\")\n",
    "                print(f\"Total Memory: {param_summary['total_bytes']:,} bytes ({param_summary['total_bytes']/1024/1024:.1f} MB)\")\n",
    "                print(f\"Layers: {len(param_summary['layers'])}\")\n",
    "            else:\n",
    "                print(\"Parameters: Available (format not recognized)\")\n",
    "        \n",
    "        if \"element_spec\" in metadata and metadata[\"element_spec\"]:\n",
    "            spec_info = parse_element_spec(str(metadata[\"element_spec\"]))\n",
    "            if spec_info.get(\"batch_size\"):\n",
    "                print(f\"Batch Size: {spec_info['batch_size']}\")\n",
    "                print(f\"Sequence Length: {spec_info['sequence_length']}\")\n",
    "                print(f\"Input Shape: {spec_info['input_shape']}\")\n",
    "                print(f\"Input Dtype: {spec_info['input_dtype']}\")\n",
    "            else:\n",
    "                print(\"Element Spec: Available\")\n",
    "        \n",
    "        if \"context_spec\" in metadata and metadata[\"context_spec\"]:\n",
    "            context_info = parse_context_spec(str(metadata[\"context_spec\"]))\n",
    "            if context_info.get(\"total_specs\"):\n",
    "                print(f\"Batch Specs: {len(context_info['batch_specs'])}\")\n",
    "                print(f\"Gradient Specs: {len(context_info['grad_specs'])}\")\n",
    "                print(f\"Total Specs: {context_info['total_specs']}\")\n",
    "            else:\n",
    "                print(\"Context Spec: Available\")\n",
    "    \n",
    "# Run the function\n",
    "print_formatted_kpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'context_spec': '| Path                                                    | '\n",
      "                 'Spec                           |\\n'\n",
      "                 '|---------------------------------------------------------|--------------------------------|\\n'\n",
      "                 '| `batch.input`                                           | '\n",
      "                 '`i32[4 200]`                   |\\n'\n",
      "                 '| `batch.loss_mask`                                       | '\n",
      "                 '`bool_[4 200 1]`               |\\n'\n",
      "                 '| `batch.target`                                          | '\n",
      "                 '`i32[4 200 1]`                 |\\n'\n",
      "                 '| `grads.embedder.input_embedding`                        | '\n",
      "                 '`bf16[262144 1152]`            |\\n'\n",
      "                 '| `grads.final_norm.scale`                                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_0.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_0.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_0.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_0.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_0.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_0.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_0.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_0.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_0.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_0.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_0.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_1.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_1.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_1.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_1.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_1.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_1.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_1.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_1.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_1.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_1.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_1.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_10.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_10.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_10.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_10.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_10.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_10.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_10.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_10.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_10.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_10.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_10.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_11.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_11.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_11.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_11.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_11.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_11.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_11.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_11.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_11.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_11.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_11.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_12.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_12.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_12.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_12.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_12.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_12.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_12.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_12.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_12.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_12.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_12.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_13.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_13.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_13.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_13.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_13.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_13.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_13.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_13.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_13.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_13.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_13.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_14.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_14.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_14.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_14.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_14.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_14.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_14.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_14.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_14.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_14.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_14.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_15.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_15.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_15.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_15.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_15.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_15.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_15.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_15.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_15.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_15.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_15.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_16.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_16.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_16.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_16.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_16.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_16.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_16.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_16.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_16.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_16.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_16.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_17.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_17.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_17.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_17.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_17.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_17.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_17.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_17.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_17.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_17.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_17.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_18.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_18.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_18.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_18.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_18.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_18.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_18.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_18.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_18.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_18.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_18.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_19.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_19.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_19.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_19.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_19.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_19.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_19.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_19.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_19.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_19.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_19.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_2.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_2.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_2.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_2.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_2.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_2.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_2.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_2.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_2.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_2.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_2.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_20.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_20.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_20.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_20.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_20.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_20.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_20.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_20.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_20.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_20.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_20.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_21.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_21.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_21.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_21.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_21.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_21.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_21.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_21.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_21.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_21.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_21.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_22.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_22.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_22.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_22.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_22.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_22.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_22.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_22.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_22.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_22.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_22.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_23.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_23.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_23.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_23.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_23.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_23.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_23.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_23.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_23.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_23.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_23.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_24.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_24.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_24.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_24.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_24.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_24.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_24.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_24.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_24.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_24.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_24.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_25.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_25.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_25.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_25.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_25.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_25.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_25.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_25.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_25.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_25.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_25.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_3.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_3.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_3.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_3.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_3.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_3.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_3.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_3.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_3.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_3.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_3.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_4.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_4.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_4.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_4.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_4.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_4.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_4.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_4.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_4.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_4.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_4.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_5.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_5.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_5.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_5.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_5.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_5.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_5.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_5.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_5.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_5.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_5.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_6.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_6.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_6.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_6.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_6.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_6.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_6.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_6.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_6.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_6.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_6.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_7.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_7.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_7.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_7.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_7.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_7.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_7.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_7.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_7.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_7.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_7.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_8.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_8.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_8.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_8.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_8.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_8.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_8.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_8.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_8.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_8.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_8.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_9.attn._key_norm.scale`                    | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_9.attn._query_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `grads.layer_9.attn.attn_vec_einsum.w`                  | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `grads.layer_9.attn.kv_einsum.w`                        | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `grads.layer_9.attn.q_einsum.w`                         | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `grads.layer_9.mlp.gating_einsum`                       | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `grads.layer_9.mlp.linear`                              | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `grads.layer_9.post_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_9.post_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_9.pre_attention_norm.scale`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads.layer_9.pre_ffw_norm.scale`                      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `interms.__call__[0].logits`                            | '\n",
      "                 '`bf16[4 200 262144]`           |\\n'\n",
      "                 '| `interms.final_norm.__call__[0]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_0.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_0.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_0.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_0.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_0.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_0.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_1.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_1.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_1.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_1.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_1.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_1.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_10.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_10.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_10.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_10.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_10.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_10.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_11.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_11.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_11.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_11.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_11.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_11.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_12.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_12.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_12.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_12.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_12.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_12.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_13.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_13.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_13.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_13.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_13.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_13.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_14.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_14.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_14.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_14.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_14.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_14.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_15.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_15.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_15.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_15.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_15.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_15.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_16.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_16.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_16.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_16.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_16.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_16.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_17.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_17.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_17.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_17.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_17.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_17.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_18.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_18.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_18.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_18.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_18.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_18.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_19.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_19.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_19.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_19.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_19.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_19.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_2.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_2.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_2.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_2.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_2.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_2.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_20.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_20.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_20.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_20.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_20.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_20.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_21.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_21.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_21.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_21.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_21.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_21.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_22.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_22.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_22.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_22.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_22.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_22.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_23.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_23.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_23.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_23.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_23.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_23.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_24.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_24.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_24.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_24.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_24.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_24.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.__call__[0][1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.attn.__call__[0][1]`                  | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.attn._key_norm.__call__[0]`           | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_25.attn._query_norm.__call__[0]`         | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_25.attn.attn_vec_einsum.__call__[0]`     | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.attn.kv_einsum.__call__[0]`           | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_25.attn.q_einsum.__call__[0]`            | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_25.mlp.__call__[0]`                      | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_25.mlp.__call__[1]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.mlp.__call__[2]`                      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.post_attention_norm.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.post_ffw_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.pre_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_25.pre_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_3.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_3.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_3.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_3.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_3.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_3.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_4.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_4.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_4.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_4.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_4.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_4.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_5.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_5.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_5.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_5.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_5.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_5.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_6.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_6.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_6.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_6.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_6.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_6.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_7.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_7.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_7.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_7.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_7.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_7.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_8.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_8.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_8.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_8.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_8.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_8.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.__call__[0][1]`                        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.attn.__call__[0][1]`                   | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.attn._key_norm.__call__[0]`            | '\n",
      "                 '`bf16[4 200 1 256]`            |\\n'\n",
      "                 '| `interms.layer_9.attn._query_norm.__call__[0]`          | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_9.attn.attn_vec_einsum.__call__[0]`      | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.attn.kv_einsum.__call__[0]`            | '\n",
      "                 '`bf16[2 4 200 1 256]`          |\\n'\n",
      "                 '| `interms.layer_9.attn.q_einsum.__call__[0]`             | '\n",
      "                 '`bf16[4 200 4 256]`            |\\n'\n",
      "                 '| `interms.layer_9.mlp.__call__[0]`                       | '\n",
      "                 '`bf16[4 200 2 6912]`           |\\n'\n",
      "                 '| `interms.layer_9.mlp.__call__[1]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.mlp.__call__[2]`                       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.post_attention_norm.__call__[0]`       | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.post_ffw_norm.__call__[0]`             | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.pre_attention_norm.__call__[0]`        | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `interms.layer_9.pre_ffw_norm.__call__[0]`              | '\n",
      "                 '`bf16[4 200 1152]`             |\\n'\n",
      "                 '| `loss_states.loss.count`                                | '\n",
      "                 '`f32[]`                        |\\n'\n",
      "                 '| `loss_states.loss.value`                                | '\n",
      "                 '`f32[]`                        |\\n'\n",
      "                 '| `loss_total`                                            | '\n",
      "                 '`f32[]`                        |\\n'\n",
      "                 '| `opt_state[0].count`                                    | '\n",
      "                 '`i32[]`                        |\\n'\n",
      "                 '| `opt_state[0].v.embedder.input_embedding`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.final_norm.scale`                       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_0.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_1.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_10.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_11.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_12.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_13.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_14.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_15.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_16.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_17.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_18.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_19.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_2.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_20.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_21.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_22.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_23.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_24.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.attn._key_norm.scale`          | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.attn._query_norm.scale`        | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.attn.attn_vec_einsum.w`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.attn.kv_einsum.w`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.attn.q_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.mlp.gating_einsum`             | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.mlp.linear`                    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.post_attention_norm.scale`     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.post_ffw_norm.scale`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.pre_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_25.pre_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_3.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_4.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_5.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_6.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_7.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_8.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.attn._key_norm.scale`           | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.attn._query_norm.scale`         | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.attn.attn_vec_einsum.w`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.attn.kv_einsum.w`               | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.attn.q_einsum.w`                | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.mlp.gating_einsum`              | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.mlp.linear`                     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.post_attention_norm.scale`      | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.post_ffw_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.pre_attention_norm.scale`       | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v.layer_9.pre_ffw_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.embedder.input_embedding`           | '\n",
      "                 '`bf16[262144]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.final_norm.scale`                   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_0.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_1.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_10.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_11.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_12.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_13.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_14.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_15.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_16.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_17.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_18.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_19.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_2.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_20.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_21.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_22.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_23.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_24.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.mlp.linear`                | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_25.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_3.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_4.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_5.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_6.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_7.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_8.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 1152]`               |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 6912]`                 |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.mlp.linear`                 | '\n",
      "                 '`bf16[6912]`                   |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_col.layer_9.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.embedder.input_embedding`           | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.final_norm.scale`                   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_0.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_1.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_10.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_11.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_12.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_13.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_14.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_15.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_16.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_17.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_18.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_19.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_2.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_20.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_21.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_22.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_23.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_24.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.attn._key_norm.scale`      | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.attn._query_norm.scale`    | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.attn.attn_vec_einsum.w`    | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.attn.kv_einsum.w`          | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.attn.q_einsum.w`           | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.mlp.gating_einsum`         | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.mlp.linear`                | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.post_attention_norm.scale` | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.post_ffw_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.pre_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_25.pre_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_3.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_4.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_5.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_6.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_7.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_8.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.attn._key_norm.scale`       | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.attn._query_norm.scale`     | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.attn.attn_vec_einsum.w`     | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.attn.kv_einsum.w`           | '\n",
      "                 '`bf16[2 1 256]`                |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.attn.q_einsum.w`            | '\n",
      "                 '`bf16[4 256]`                  |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.mlp.gating_einsum`          | '\n",
      "                 '`bf16[2 1152]`                 |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.mlp.linear`                 | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.post_attention_norm.scale`  | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.post_ffw_norm.scale`        | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.pre_attention_norm.scale`   | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `opt_state[0].v_row.layer_9.pre_ffw_norm.scale`         | '\n",
      "                 '`bf16[1]`                      |\\n'\n",
      "                 '| `params.embedder.input_embedding`                       | '\n",
      "                 '`bf16[262144 1152]`            |\\n'\n",
      "                 '| `params.final_norm.scale`                               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_0.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_0.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_0.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_0.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_0.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_0.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_0.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_0.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_0.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_0.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_0.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_1.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_1.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_1.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_1.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_1.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_1.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_1.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_1.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_1.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_1.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_1.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_10.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_10.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_10.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_10.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_10.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_10.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_10.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_10.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_10.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_10.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_10.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_11.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_11.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_11.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_11.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_11.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_11.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_11.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_11.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_11.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_11.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_11.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_12.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_12.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_12.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_12.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_12.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_12.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_12.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_12.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_12.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_12.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_12.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_13.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_13.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_13.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_13.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_13.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_13.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_13.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_13.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_13.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_13.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_13.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_14.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_14.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_14.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_14.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_14.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_14.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_14.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_14.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_14.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_14.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_14.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_15.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_15.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_15.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_15.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_15.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_15.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_15.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_15.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_15.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_15.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_15.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_16.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_16.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_16.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_16.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_16.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_16.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_16.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_16.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_16.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_16.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_16.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_17.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_17.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_17.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_17.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_17.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_17.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_17.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_17.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_17.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_17.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_17.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_18.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_18.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_18.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_18.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_18.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_18.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_18.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_18.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_18.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_18.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_18.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_19.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_19.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_19.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_19.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_19.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_19.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_19.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_19.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_19.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_19.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_19.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_2.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_2.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_2.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_2.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_2.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_2.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_2.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_2.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_2.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_2.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_2.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_20.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_20.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_20.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_20.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_20.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_20.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_20.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_20.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_20.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_20.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_20.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_21.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_21.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_21.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_21.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_21.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_21.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_21.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_21.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_21.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_21.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_21.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_22.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_22.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_22.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_22.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_22.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_22.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_22.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_22.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_22.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_22.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_22.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_23.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_23.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_23.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_23.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_23.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_23.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_23.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_23.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_23.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_23.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_23.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_24.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_24.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_24.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_24.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_24.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_24.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_24.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_24.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_24.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_24.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_24.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_25.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_25.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_25.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_25.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_25.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_25.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_25.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_25.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_25.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_25.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_25.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_3.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_3.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_3.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_3.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_3.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_3.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_3.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_3.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_3.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_3.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_3.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_4.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_4.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_4.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_4.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_4.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_4.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_4.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_4.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_4.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_4.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_4.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_5.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_5.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_5.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_5.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_5.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_5.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_5.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_5.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_5.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_5.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_5.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_6.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_6.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_6.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_6.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_6.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_6.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_6.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_6.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_6.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_6.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_6.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_7.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_7.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_7.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_7.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_7.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_7.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_7.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_7.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_7.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_7.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_7.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_8.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_8.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_8.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_8.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_8.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_8.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_8.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_8.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_8.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_8.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_8.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_9.attn._key_norm.scale`                   | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_9.attn._query_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `params.layer_9.attn.attn_vec_einsum.w`                 | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `params.layer_9.attn.kv_einsum.w`                       | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `params.layer_9.attn.q_einsum.w`                        | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `params.layer_9.mlp.gating_einsum`                      | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `params.layer_9.mlp.linear`                             | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `params.layer_9.post_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_9.post_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_9.pre_attention_norm.scale`               | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `params.layer_9.pre_ffw_norm.scale`                     | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `preds.logits`                                          | '\n",
      "                 '`bf16[4 200 262144]`           |\\n'\n",
      "                 '| `step`                                                  | '\n",
      "                 '`i32[]`                        |\\n'\n",
      "                 '| `updates.embedder.input_embedding`                      | '\n",
      "                 '`bf16[262144 1152]`            |\\n'\n",
      "                 '| `updates.final_norm.scale`                              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_0.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_0.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_0.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_0.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_0.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_0.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_0.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_0.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_0.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_0.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_0.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_1.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_1.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_1.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_1.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_1.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_1.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_1.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_1.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_1.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_1.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_1.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_10.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_10.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_10.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_10.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_10.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_10.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_10.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_10.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_10.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_10.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_10.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_11.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_11.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_11.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_11.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_11.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_11.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_11.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_11.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_11.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_11.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_11.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_12.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_12.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_12.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_12.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_12.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_12.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_12.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_12.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_12.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_12.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_12.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_13.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_13.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_13.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_13.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_13.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_13.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_13.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_13.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_13.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_13.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_13.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_14.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_14.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_14.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_14.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_14.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_14.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_14.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_14.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_14.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_14.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_14.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_15.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_15.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_15.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_15.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_15.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_15.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_15.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_15.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_15.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_15.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_15.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_16.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_16.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_16.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_16.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_16.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_16.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_16.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_16.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_16.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_16.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_16.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_17.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_17.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_17.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_17.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_17.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_17.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_17.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_17.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_17.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_17.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_17.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_18.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_18.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_18.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_18.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_18.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_18.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_18.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_18.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_18.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_18.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_18.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_19.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_19.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_19.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_19.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_19.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_19.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_19.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_19.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_19.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_19.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_19.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_2.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_2.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_2.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_2.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_2.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_2.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_2.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_2.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_2.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_2.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_2.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_20.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_20.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_20.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_20.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_20.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_20.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_20.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_20.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_20.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_20.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_20.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_21.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_21.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_21.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_21.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_21.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_21.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_21.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_21.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_21.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_21.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_21.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_22.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_22.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_22.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_22.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_22.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_22.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_22.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_22.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_22.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_22.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_22.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_23.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_23.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_23.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_23.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_23.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_23.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_23.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_23.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_23.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_23.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_23.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_24.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_24.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_24.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_24.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_24.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_24.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_24.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_24.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_24.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_24.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_24.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_25.attn._key_norm.scale`                 | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_25.attn._query_norm.scale`               | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_25.attn.attn_vec_einsum.w`               | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_25.attn.kv_einsum.w`                     | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_25.attn.q_einsum.w`                      | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_25.mlp.gating_einsum`                    | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_25.mlp.linear`                           | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_25.post_attention_norm.scale`            | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_25.post_ffw_norm.scale`                  | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_25.pre_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_25.pre_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_3.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_3.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_3.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_3.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_3.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_3.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_3.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_3.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_3.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_3.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_3.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_4.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_4.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_4.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_4.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_4.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_4.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_4.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_4.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_4.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_4.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_4.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_5.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_5.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_5.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_5.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_5.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_5.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_5.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_5.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_5.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_5.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_5.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_6.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_6.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_6.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_6.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_6.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_6.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_6.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_6.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_6.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_6.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_6.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_7.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_7.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_7.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_7.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_7.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_7.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_7.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_7.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_7.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_7.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_7.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_8.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_8.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_8.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_8.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_8.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_8.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_8.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_8.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_8.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_8.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_8.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_9.attn._key_norm.scale`                  | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_9.attn._query_norm.scale`                | '\n",
      "                 '`bf16[256]`                    |\\n'\n",
      "                 '| `updates.layer_9.attn.attn_vec_einsum.w`                | '\n",
      "                 '`bf16[4 256 1152]`             |\\n'\n",
      "                 '| `updates.layer_9.attn.kv_einsum.w`                      | '\n",
      "                 '`bf16[2 1 1152 256]`           |\\n'\n",
      "                 '| `updates.layer_9.attn.q_einsum.w`                       | '\n",
      "                 '`bf16[4 1152 256]`             |\\n'\n",
      "                 '| `updates.layer_9.mlp.gating_einsum`                     | '\n",
      "                 '`bf16[2 6912 1152]`            |\\n'\n",
      "                 '| `updates.layer_9.mlp.linear`                            | '\n",
      "                 '`bf16[6912 1152]`              |\\n'\n",
      "                 '| `updates.layer_9.post_attention_norm.scale`             | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_9.post_ffw_norm.scale`                   | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_9.pre_attention_norm.scale`              | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `updates.layer_9.pre_ffw_norm.scale`                    | '\n",
      "                 '`bf16[1152]`                   |\\n'\n",
      "                 '| `grads`                                                 | '\n",
      "                 '`<<same structure as params>>` |\\n'\n",
      "                 '| `updates`                                               | '\n",
      "                 '`<<same structure as params>>` |',\n",
      " 'element_spec': '```python\\n'\n",
      "                 '{\\n'\n",
      "                 '  \"input\": {\\n'\n",
      "                 '    \"shape\": [\\n'\n",
      "                 '      4,\\n'\n",
      "                 '      200\\n'\n",
      "                 '    ],\\n'\n",
      "                 '    \"dtype\": \"int64\"\\n'\n",
      "                 '  },\\n'\n",
      "                 '  \"loss_mask\": {\\n'\n",
      "                 '    \"shape\": [\\n'\n",
      "                 '      4,\\n'\n",
      "                 '      200,\\n'\n",
      "                 '      1\\n'\n",
      "                 '    ],\\n'\n",
      "                 '    \"dtype\": \"bool\"\\n'\n",
      "                 '  },\\n'\n",
      "                 '  \"target\": {\\n'\n",
      "                 '    \"shape\": [\\n'\n",
      "                 '      4,\\n'\n",
      "                 '      200,\\n'\n",
      "                 '      1\\n'\n",
      "                 '    ],\\n'\n",
      "                 '    \"dtype\": \"int64\"\\n'\n",
      "                 '  }\\n'\n",
      "                 '}\\n'\n",
      "                 '```',\n",
      " 'num_params': np.int64(999885952),\n",
      " 'parameters': '| Name                               | Shape             | '\n",
      "               'Dtype    | Size        | Mean      | Std     | Sharding |\\n'\n",
      "               '|------------------------------------|-------------------|----------|-------------|-----------|---------|----------|\\n'\n",
      "               '| embedder/input_embedding           | (262144, 1152)    | '\n",
      "               'bfloat16 | 301,989,888 | -0.000148 | 0.028   | ()       |\\n'\n",
      "               '| final_norm/scale                   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 7.03      | 5.25    | ()       |\\n'\n",
      "               '| layer_0/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.457     | 0.672   | ()       |\\n'\n",
      "               '| layer_0/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.49      | 0.559   | ()       |\\n'\n",
      "               '| layer_0/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 4.91e-05  | 0.0236  | ()       |\\n'\n",
      "               '| layer_0/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -3.24e-05 | 0.0408  | ()       |\\n'\n",
      "               '| layer_0/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.03e-05 | 0.0216  | ()       |\\n'\n",
      "               '| layer_0/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 4.51e-07  | 0.031   | ()       |\\n'\n",
      "               '| layer_0/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 7.75e-06  | 0.0125  | ()       |\\n'\n",
      "               '| layer_0/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | -0.0635   | 2.47    | ()       |\\n'\n",
      "               '| layer_0/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.89      | 5.53    | ()       |\\n'\n",
      "               '| layer_0/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.56      | 2.77    | ()       |\\n'\n",
      "               '| layer_0/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.0       | 2.11    | ()       |\\n'\n",
      "               '| layer_1/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.793     | 0.637   | ()       |\\n'\n",
      "               '| layer_1/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.781     | 0.574   | ()       |\\n'\n",
      "               '| layer_1/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 8.7e-06   | 0.0227  | ()       |\\n'\n",
      "               '| layer_1/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -5.87e-05 | 0.0432  | ()       |\\n'\n",
      "               '| layer_1/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 7.42e-06  | 0.0203  | ()       |\\n'\n",
      "               '| layer_1/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.15e-05 | 0.0297  | ()       |\\n'\n",
      "               '| layer_1/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -5.45e-06 | 0.0123  | ()       |\\n'\n",
      "               '| layer_1/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | -0.249    | 1.69    | ()       |\\n'\n",
      "               '| layer_1/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.695     | 1.88    | ()       |\\n'\n",
      "               '| layer_1/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 12.1      | 6.31    | ()       |\\n'\n",
      "               '| layer_1/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 7.53      | 3.53    | ()       |\\n'\n",
      "               '| layer_10/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.738     | 1.94    | ()       |\\n'\n",
      "               '| layer_10/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.402     | 0.91    | ()       |\\n'\n",
      "               '| layer_10/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.39e-05  | 0.0198  | ()       |\\n'\n",
      "               '| layer_10/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 1.75e-05  | 0.0332  | ()       |\\n'\n",
      "               '| layer_10/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -2.55e-05 | 0.0267  | ()       |\\n'\n",
      "               '| layer_10/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 7.39e-05  | 0.0305  | ()       |\\n'\n",
      "               '| layer_10/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 5.07e-06  | 0.0118  | ()       |\\n'\n",
      "               '| layer_10/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 9.44      | 7.66    | ()       |\\n'\n",
      "               '| layer_10/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 9.19      | 6.12    | ()       |\\n'\n",
      "               '| layer_10/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.41      | 2.48    | ()       |\\n'\n",
      "               '| layer_10/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.34      | 1.27    | ()       |\\n'\n",
      "               '| layer_11/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 1.2       | 1.18    | ()       |\\n'\n",
      "               '| layer_11/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.723     | 0.805   | ()       |\\n'\n",
      "               '| layer_11/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -2.93e-05 | 0.0211  | ()       |\\n'\n",
      "               '| layer_11/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -3.81e-05 | 0.0327  | ()       |\\n'\n",
      "               '| layer_11/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -9.44e-05 | 0.027   | ()       |\\n'\n",
      "               '| layer_11/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 6.15e-05  | 0.0284  | ()       |\\n'\n",
      "               '| layer_11/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -2.47e-06 | 0.0107  | ()       |\\n'\n",
      "               '| layer_11/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 15.1      | 9.94    | ()       |\\n'\n",
      "               '| layer_11/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 13.4      | 7.5     | ()       |\\n'\n",
      "               '| layer_11/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.58      | 1.97    | ()       |\\n'\n",
      "               '| layer_11/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.25      | 1.41    | ()       |\\n'\n",
      "               '| layer_12/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.688     | 1.2     | ()       |\\n'\n",
      "               '| layer_12/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.648     | 0.805   | ()       |\\n'\n",
      "               '| layer_12/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 7.3e-06   | 0.0223  | ()       |\\n'\n",
      "               '| layer_12/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 9.82e-05  | 0.0518  | ()       |\\n'\n",
      "               '| layer_12/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 2.47e-05  | 0.0171  | ()       |\\n'\n",
      "               '| layer_12/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 3.79e-05  | 0.0295  | ()       |\\n'\n",
      "               '| layer_12/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 4.62e-06  | 0.00787 | ()       |\\n'\n",
      "               '| layer_12/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 12.6      | 6.88    | ()       |\\n'\n",
      "               '| layer_12/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 16.2      | 8.19    | ()       |\\n'\n",
      "               '| layer_12/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.56      | 2.52    | ()       |\\n'\n",
      "               '| layer_12/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.02      | 1.02    | ()       |\\n'\n",
      "               '| layer_13/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.801     | 1.05    | ()       |\\n'\n",
      "               '| layer_13/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.676     | 0.809   | ()       |\\n'\n",
      "               '| layer_13/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 6.79e-06  | 0.0219  | ()       |\\n'\n",
      "               '| layer_13/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 0.000128  | 0.0466  | ()       |\\n'\n",
      "               '| layer_13/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.05e-05 | 0.0188  | ()       |\\n'\n",
      "               '| layer_13/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 5.34e-05  | 0.0286  | ()       |\\n'\n",
      "               '| layer_13/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 1.04e-06  | 0.00711 | ()       |\\n'\n",
      "               '| layer_13/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 16.6      | 8.94    | ()       |\\n'\n",
      "               '| layer_13/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 21.5      | 10.2    | ()       |\\n'\n",
      "               '| layer_13/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.09      | 2.5     | ()       |\\n'\n",
      "               '| layer_13/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.82      | 1.0     | ()       |\\n'\n",
      "               '| layer_14/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.539     | 0.77    | ()       |\\n'\n",
      "               '| layer_14/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.484     | 0.633   | ()       |\\n'\n",
      "               '| layer_14/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -6.26e-06 | 0.0248  | ()       |\\n'\n",
      "               '| layer_14/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 1.29e-05  | 0.0393  | ()       |\\n'\n",
      "               '| layer_14/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -9.83e-06 | 0.0223  | ()       |\\n'\n",
      "               '| layer_14/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 8.06e-05  | 0.0309  | ()       |\\n'\n",
      "               '| layer_14/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -3.1e-06  | 0.00806 | ()       |\\n'\n",
      "               '| layer_14/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 16.5      | 8.56    | ()       |\\n'\n",
      "               '| layer_14/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 27.5      | 12.7    | ()       |\\n'\n",
      "               '| layer_14/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.0       | 2.62    | ()       |\\n'\n",
      "               '| layer_14/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.41      | 1.02    | ()       |\\n'\n",
      "               '| layer_15/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.652     | 1.05    | ()       |\\n'\n",
      "               '| layer_15/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.535     | 0.723   | ()       |\\n'\n",
      "               '| layer_15/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 8.05e-06  | 0.0233  | ()       |\\n'\n",
      "               '| layer_15/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 7.49e-05  | 0.0518  | ()       |\\n'\n",
      "               '| layer_15/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -5.19e-06 | 0.0171  | ()       |\\n'\n",
      "               '| layer_15/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 6.63e-05  | 0.0283  | ()       |\\n'\n",
      "               '| layer_15/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -3.19e-07 | 0.0121  | ()       |\\n'\n",
      "               '| layer_15/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 21.1      | 11.0    | ()       |\\n'\n",
      "               '| layer_15/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 34.0      | 15.9    | ()       |\\n'\n",
      "               '| layer_15/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.16      | 2.78    | ()       |\\n'\n",
      "               '| layer_15/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.49      | 1.21    | ()       |\\n'\n",
      "               '| layer_16/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.734     | 1.41    | ()       |\\n'\n",
      "               '| layer_16/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.479     | 0.875   | ()       |\\n'\n",
      "               '| layer_16/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 9.3e-06   | 0.0159  | ()       |\\n'\n",
      "               '| layer_16/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 2.6e-05   | 0.0396  | ()       |\\n'\n",
      "               '| layer_16/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -4.79e-05 | 0.0223  | ()       |\\n'\n",
      "               '| layer_16/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 6.34e-05  | 0.0311  | ()       |\\n'\n",
      "               '| layer_16/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 1.46e-06  | 0.0115  | ()       |\\n'\n",
      "               '| layer_16/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 33.0      | 16.6    | ()       |\\n'\n",
      "               '| layer_16/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 51.0      | 22.9    | ()       |\\n'\n",
      "               '| layer_16/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.47      | 3.12    | ()       |\\n'\n",
      "               '| layer_16/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.48      | 1.66    | ()       |\\n'\n",
      "               '| layer_17/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 1.05      | 1.23    | ()       |\\n'\n",
      "               '| layer_17/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.605     | 0.855   | ()       |\\n'\n",
      "               '| layer_17/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.85e-05 | 0.0177  | ()       |\\n'\n",
      "               '| layer_17/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 5.41e-05  | 0.0342  | ()       |\\n'\n",
      "               '| layer_17/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 4.89e-05  | 0.0258  | ()       |\\n'\n",
      "               '| layer_17/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -3.47e-06 | 0.0294  | ()       |\\n'\n",
      "               '| layer_17/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -3.78e-06 | 0.00928 | ()       |\\n'\n",
      "               '| layer_17/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 54.0      | 24.4    | ()       |\\n'\n",
      "               '| layer_17/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 56.0      | 23.9    | ()       |\\n'\n",
      "               '| layer_17/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.66      | 2.92    | ()       |\\n'\n",
      "               '| layer_17/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.46      | 1.66    | ()       |\\n'\n",
      "               '| layer_18/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.342     | 0.758   | ()       |\\n'\n",
      "               '| layer_18/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.508     | 0.746   | ()       |\\n'\n",
      "               '| layer_18/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -8.52e-06 | 0.0148  | ()       |\\n'\n",
      "               '| layer_18/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -8.05e-06 | 0.0491  | ()       |\\n'\n",
      "               '| layer_18/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.07e-06 | 0.0178  | ()       |\\n'\n",
      "               '| layer_18/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.79e-05 | 0.0305  | ()       |\\n'\n",
      "               '| layer_18/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.9e-06  | 0.00647 | ()       |\\n'\n",
      "               '| layer_18/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 38.5      | 19.9    | ()       |\\n'\n",
      "               '| layer_18/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 60.2      | 25.1    | ()       |\\n'\n",
      "               '| layer_18/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.83      | 4.88    | ()       |\\n'\n",
      "               '| layer_18/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.08      | 1.6     | ()       |\\n'\n",
      "               '| layer_19/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.707     | 1.19    | ()       |\\n'\n",
      "               '| layer_19/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.586     | 0.781   | ()       |\\n'\n",
      "               '| layer_19/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.78e-05 | 0.0262  | ()       |\\n'\n",
      "               '| layer_19/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -0.000103 | 0.0332  | ()       |\\n'\n",
      "               '| layer_19/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -4.01e-05 | 0.0265  | ()       |\\n'\n",
      "               '| layer_19/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.69e-05 | 0.0303  | ()       |\\n'\n",
      "               '| layer_19/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.63e-06 | 0.00778 | ()       |\\n'\n",
      "               '| layer_19/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 35.2      | 15.8    | ()       |\\n'\n",
      "               '| layer_19/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 61.2      | 26.6    | ()       |\\n'\n",
      "               '| layer_19/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.28      | 4.59    | ()       |\\n'\n",
      "               '| layer_19/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.05      | 1.52    | ()       |\\n'\n",
      "               '| layer_2/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.527     | 1.66    | ()       |\\n'\n",
      "               '| layer_2/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.371     | 0.523   | ()       |\\n'\n",
      "               '| layer_2/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.74e-05  | 0.0226  | ()       |\\n'\n",
      "               '| layer_2/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -0.000121 | 0.0515  | ()       |\\n'\n",
      "               '| layer_2/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.18e-05  | 0.0171  | ()       |\\n'\n",
      "               '| layer_2/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 4.17e-05  | 0.0297  | ()       |\\n'\n",
      "               '| layer_2/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 4.64e-07  | 0.0119  | ()       |\\n'\n",
      "               '| layer_2/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | -0.00641  | 2.12    | ()       |\\n'\n",
      "               '| layer_2/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.36      | 3.27    | ()       |\\n'\n",
      "               '| layer_2/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 12.3      | 5.44    | ()       |\\n'\n",
      "               '| layer_2/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 7.75      | 3.7     | ()       |\\n'\n",
      "               '| layer_20/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.543     | 0.836   | ()       |\\n'\n",
      "               '| layer_20/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.508     | 0.672   | ()       |\\n'\n",
      "               '| layer_20/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -4.43e-07 | 0.0234  | ()       |\\n'\n",
      "               '| layer_20/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -1.07e-05 | 0.0425  | ()       |\\n'\n",
      "               '| layer_20/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.29e-05 | 0.0208  | ()       |\\n'\n",
      "               '| layer_20/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.01e-05 | 0.0286  | ()       |\\n'\n",
      "               '| layer_20/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.21e-06 | 0.0114  | ()       |\\n'\n",
      "               '| layer_20/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 43.0      | 22.6    | ()       |\\n'\n",
      "               '| layer_20/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 66.5      | 29.8    | ()       |\\n'\n",
      "               '| layer_20/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.09      | 3.86    | ()       |\\n'\n",
      "               '| layer_20/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.969     | 1.48    | ()       |\\n'\n",
      "               '| layer_21/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.586     | 0.723   | ()       |\\n'\n",
      "               '| layer_21/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.605     | 0.613   | ()       |\\n'\n",
      "               '| layer_21/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 7.15e-06  | 0.0282  | ()       |\\n'\n",
      "               '| layer_21/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -8.34e-06 | 0.0549  | ()       |\\n'\n",
      "               '| layer_21/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.7e-05   | 0.016   | ()       |\\n'\n",
      "               '| layer_21/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.71e-06 | 0.0298  | ()       |\\n'\n",
      "               '| layer_21/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 4.89e-06  | 0.00778 | ()       |\\n'\n",
      "               '| layer_21/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 36.0      | 15.8    | ()       |\\n'\n",
      "               '| layer_21/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 70.0      | 31.6    | ()       |\\n'\n",
      "               '| layer_21/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.25      | 6.25    | ()       |\\n'\n",
      "               '| layer_21/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.898     | 1.47    | ()       |\\n'\n",
      "               '| layer_22/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.84      | 1.03    | ()       |\\n'\n",
      "               '| layer_22/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.766     | 0.816   | ()       |\\n'\n",
      "               '| layer_22/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 8.4e-06   | 0.0215  | ()       |\\n'\n",
      "               '| layer_22/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 2.88e-05  | 0.0493  | ()       |\\n'\n",
      "               '| layer_22/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -4.32e-05 | 0.0179  | ()       |\\n'\n",
      "               '| layer_22/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -1.88e-05 | 0.0298  | ()       |\\n'\n",
      "               '| layer_22/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -5.25e-06 | 0.0107  | ()       |\\n'\n",
      "               '| layer_22/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 45.2      | 21.4    | ()       |\\n'\n",
      "               '| layer_22/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 75.5      | 34.2    | ()       |\\n'\n",
      "               '| layer_22/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.03      | 6.09    | ()       |\\n'\n",
      "               '| layer_22/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.789     | 1.42    | ()       |\\n'\n",
      "               '| layer_23/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.777     | 1.08    | ()       |\\n'\n",
      "               '| layer_23/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.229     | 0.949   | ()       |\\n'\n",
      "               '| layer_23/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.05e-05  | 0.0276  | ()       |\\n'\n",
      "               '| layer_23/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -3.62e-05 | 0.0552  | ()       |\\n'\n",
      "               '| layer_23/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -2.09e-05 | 0.0161  | ()       |\\n'\n",
      "               '| layer_23/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -3.29e-05 | 0.0299  | ()       |\\n'\n",
      "               '| layer_23/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 1.68e-06  | 0.0121  | ()       |\\n'\n",
      "               '| layer_23/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 36.2      | 16.2    | ()       |\\n'\n",
      "               '| layer_23/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 85.5      | 34.2    | ()       |\\n'\n",
      "               '| layer_23/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.55      | 4.75    | ()       |\\n'\n",
      "               '| layer_23/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.379     | 1.12    | ()       |\\n'\n",
      "               '| layer_24/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.625     | 0.91    | ()       |\\n'\n",
      "               '| layer_24/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.59      | 0.637   | ()       |\\n'\n",
      "               '| layer_24/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.57e-05 | 0.0167  | ()       |\\n'\n",
      "               '| layer_24/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 1.17e-05  | 0.0417  | ()       |\\n'\n",
      "               '| layer_24/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -7.06e-06 | 0.0211  | ()       |\\n'\n",
      "               '| layer_24/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -2.43e-05 | 0.0309  | ()       |\\n'\n",
      "               '| layer_24/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.1e-06  | 0.00946 | ()       |\\n'\n",
      "               '| layer_24/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 44.8      | 18.5    | ()       |\\n'\n",
      "               '| layer_24/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.06e+02  | 42.2    | ()       |\\n'\n",
      "               '| layer_24/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.0       | 3.95    | ()       |\\n'\n",
      "               '| layer_24/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.153     | 0.859   | ()       |\\n'\n",
      "               '| layer_25/attn/_key_norm/scale      | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.66      | 0.789   | ()       |\\n'\n",
      "               '| layer_25/attn/_query_norm/scale    | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.668     | 0.68    | ()       |\\n'\n",
      "               '| layer_25/attn/attn_vec_einsum/w    | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 9.78e-06  | 0.0237  | ()       |\\n'\n",
      "               '| layer_25/attn/kv_einsum/w          | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -5.75e-05 | 0.0309  | ()       |\\n'\n",
      "               '| layer_25/attn/q_einsum/w           | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -3.61e-07 | 0.0284  | ()       |\\n'\n",
      "               '| layer_25/mlp/gating_einsum         | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | -5.36e-05 | 0.0308  | ()       |\\n'\n",
      "               '| layer_25/mlp/linear                | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -8.61e-07 | 0.00677 | ()       |\\n'\n",
      "               '| layer_25/post_attention_norm/scale | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 32.2      | 12.8    | ()       |\\n'\n",
      "               '| layer_25/post_ffw_norm/scale       | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 86.0      | 33.8    | ()       |\\n'\n",
      "               '| layer_25/pre_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.91      | 3.7     | ()       |\\n'\n",
      "               '| layer_25/pre_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.75      | 1.41    | ()       |\\n'\n",
      "               '| layer_3/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.369     | 0.625   | ()       |\\n'\n",
      "               '| layer_3/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.367     | 0.426   | ()       |\\n'\n",
      "               '| layer_3/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.31e-05 | 0.0209  | ()       |\\n'\n",
      "               '| layer_3/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -7.45e-06 | 0.0383  | ()       |\\n'\n",
      "               '| layer_3/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.38e-05 | 0.0229  | ()       |\\n'\n",
      "               '| layer_3/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 1.36e-05  | 0.0291  | ()       |\\n'\n",
      "               '| layer_3/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.88e-06 | 0.00946 | ()       |\\n'\n",
      "               '| layer_3/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 1.23      | 2.84    | ()       |\\n'\n",
      "               '| layer_3/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.3       | 4.25    | ()       |\\n'\n",
      "               '| layer_3/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 10.2      | 4.41    | ()       |\\n'\n",
      "               '| layer_3/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 9.12      | 4.28    | ()       |\\n'\n",
      "               '| layer_4/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.412     | 1.09    | ()       |\\n'\n",
      "               '| layer_4/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.307     | 0.758   | ()       |\\n'\n",
      "               '| layer_4/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 3.08e-05  | 0.0161  | ()       |\\n'\n",
      "               '| layer_4/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -1.16e-05 | 0.0337  | ()       |\\n'\n",
      "               '| layer_4/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 2.38e-05  | 0.0261  | ()       |\\n'\n",
      "               '| layer_4/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 9.39e-05  | 0.0293  | ()       |\\n'\n",
      "               '| layer_4/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 7.26e-07  | 0.00665 | ()       |\\n'\n",
      "               '| layer_4/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 0.914     | 2.95    | ()       |\\n'\n",
      "               '| layer_4/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.28      | 5.75    | ()       |\\n'\n",
      "               '| layer_4/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 9.69      | 4.44    | ()       |\\n'\n",
      "               '| layer_4/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.22      | 3.33    | ()       |\\n'\n",
      "               '| layer_5/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.727     | 1.11    | ()       |\\n'\n",
      "               '| layer_5/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.676     | 0.805   | ()       |\\n'\n",
      "               '| layer_5/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 4.71e-06  | 0.0271  | ()       |\\n'\n",
      "               '| layer_5/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 6.58e-05  | 0.0432  | ()       |\\n'\n",
      "               '| layer_5/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.65e-05 | 0.0205  | ()       |\\n'\n",
      "               '| layer_5/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 0.000117  | 0.0286  | ()       |\\n'\n",
      "               '| layer_5/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -4.88e-07 | 0.0123  | ()       |\\n'\n",
      "               '| layer_5/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.45      | 5.12    | ()       |\\n'\n",
      "               '| layer_5/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.33      | 5.38    | ()       |\\n'\n",
      "               '| layer_5/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.11      | 1.19    | ()       |\\n'\n",
      "               '| layer_5/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 7.78      | 4.5     | ()       |\\n'\n",
      "               '| layer_6/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.52      | 0.785   | ()       |\\n'\n",
      "               '| layer_6/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.436     | 0.594   | ()       |\\n'\n",
      "               '| layer_6/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 1.81e-05  | 0.0255  | ()       |\\n'\n",
      "               '| layer_6/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 6.68e-05  | 0.0496  | ()       |\\n'\n",
      "               '| layer_6/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -3e-05    | 0.0177  | ()       |\\n'\n",
      "               '| layer_6/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 0.000115  | 0.0304  | ()       |\\n'\n",
      "               '| layer_6/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 6.77e-06  | 0.012   | ()       |\\n'\n",
      "               '| layer_6/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 2.06      | 3.23    | ()       |\\n'\n",
      "               '| layer_6/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.03      | 5.94    | ()       |\\n'\n",
      "               '| layer_6/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 9.06      | 4.97    | ()       |\\n'\n",
      "               '| layer_6/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.16      | 3.53    | ()       |\\n'\n",
      "               '| layer_7/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.471     | 0.75    | ()       |\\n'\n",
      "               '| layer_7/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.539     | 0.602   | ()       |\\n'\n",
      "               '| layer_7/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -1.39e-05 | 0.03    | ()       |\\n'\n",
      "               '| layer_7/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | -4.82e-05 | 0.0437  | ()       |\\n'\n",
      "               '| layer_7/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 7.87e-06  | 0.0201  | ()       |\\n'\n",
      "               '| layer_7/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 0.000112  | 0.031   | ()       |\\n'\n",
      "               '| layer_7/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -4.08e-06 | 0.00989 | ()       |\\n'\n",
      "               '| layer_7/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.09      | 3.97    | ()       |\\n'\n",
      "               '| layer_7/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.06      | 7.88    | ()       |\\n'\n",
      "               '| layer_7/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.81      | 3.48    | ()       |\\n'\n",
      "               '| layer_7/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.59      | 2.56    | ()       |\\n'\n",
      "               '| layer_8/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.562     | 0.742   | ()       |\\n'\n",
      "               '| layer_8/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.527     | 0.539   | ()       |\\n'\n",
      "               '| layer_8/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | 3.22e-05  | 0.0302  | ()       |\\n'\n",
      "               '| layer_8/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 7.3e-06   | 0.0422  | ()       |\\n'\n",
      "               '| layer_8/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | 2.32e-05  | 0.0209  | ()       |\\n'\n",
      "               '| layer_8/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 9.97e-05  | 0.0304  | ()       |\\n'\n",
      "               '| layer_8/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | 1.59e-06  | 0.00659 | ()       |\\n'\n",
      "               '| layer_8/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 4.22      | 4.62    | ()       |\\n'\n",
      "               '| layer_8/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.47      | 6.0     | ()       |\\n'\n",
      "               '| layer_8/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.41      | 3.38    | ()       |\\n'\n",
      "               '| layer_8/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.53      | 1.91    | ()       |\\n'\n",
      "               '| layer_9/attn/_key_norm/scale       | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.652     | 0.832   | ()       |\\n'\n",
      "               '| layer_9/attn/_query_norm/scale     | (256,)            | '\n",
      "               'bfloat16 | 256         | 0.586     | 0.625   | ()       |\\n'\n",
      "               '| layer_9/attn/attn_vec_einsum/w     | (4, 256, 1152)    | '\n",
      "               'bfloat16 | 1,179,648   | -6.17e-06 | 0.0179  | ()       |\\n'\n",
      "               '| layer_9/attn/kv_einsum/w           | (2, 1, 1152, 256) | '\n",
      "               'bfloat16 | 589,824     | 0.000151  | 0.0518  | ()       |\\n'\n",
      "               '| layer_9/attn/q_einsum/w            | (4, 1152, 256)    | '\n",
      "               'bfloat16 | 1,179,648   | -2.77e-05 | 0.0171  | ()       |\\n'\n",
      "               '| layer_9/mlp/gating_einsum          | (2, 6912, 1152)   | '\n",
      "               'bfloat16 | 15,925,248  | 8.44e-05  | 0.0286  | ()       |\\n'\n",
      "               '| layer_9/mlp/linear                 | (6912, 1152)      | '\n",
      "               'bfloat16 | 7,962,624   | -1.13e-05 | 0.0122  | ()       |\\n'\n",
      "               '| layer_9/post_attention_norm/scale  | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 5.06      | 4.44    | ()       |\\n'\n",
      "               '| layer_9/post_ffw_norm/scale        | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 7.97      | 7.44    | ()       |\\n'\n",
      "               '| layer_9/pre_attention_norm/scale   | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 6.12      | 3.25    | ()       |\\n'\n",
      "               '| layer_9/pre_ffw_norm/scale         | (1152,)           | '\n",
      "               'bfloat16 | 1,152       | 3.86      | 1.84    | ()       |\\n'\n",
      "               '\\n'\n",
      "               'Total: 999,885,952 -- 1,999,771,904 bytes'}\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "\n",
    "pprint.pprint(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìä KEY PERFORMANCE INDICATORS\n",
      "============================================================\n",
      "\n",
      "üèóÔ∏è MODEL INFORMATION\n",
      "------------------------------\n",
      "Model Parameters: 999,885,952\n",
      "Total Parameters: 999,885,952\n",
      "Total Memory: 1,999,771,904 bytes (1907.1 MB)\n",
      "Layers: 286\n",
      "Input Spec: None\n",
      "Context Spec: Available\n"
     ]
    }
   ],
   "source": [
    "def parse_parameter_summary(param_text: str) -> dict:\n",
    "    \"\"\"Parse parameter summary text and extract key information.\"\"\"\n",
    "    if not param_text or not isinstance(param_text, str):\n",
    "        return {}\n",
    "\n",
    "    result = {\n",
    "        \"total_params\": None,\n",
    "        \"total_bytes\": None,\n",
    "        \"layers\": [],\n",
    "        \"parameter_count\": 0,\n",
    "    }\n",
    "\n",
    "    lines = param_text.split(\"\\n\")\n",
    "\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if not line or line.startswith(\"|--\") or line.startswith(\"===\"):\n",
    "            continue\n",
    "\n",
    "        # Extract total information\n",
    "        if \"Total:\" in line:\n",
    "            # Format: \"Total: 999,885,952 -- 1,999,771,904 bytes\"\n",
    "            parts = line.split(\"Total:\")[1].strip()\n",
    "            if \"--\" in parts:\n",
    "                params_part, bytes_part = parts.split(\"--\")\n",
    "                result[\"total_params\"] = int(\n",
    "                    params_part.strip().replace(\",\", \"\")\n",
    "                )\n",
    "                result[\"total_bytes\"] = int(\n",
    "                    bytes_part.strip().replace(\",\", \"\").replace(\" bytes\", \"\")\n",
    "                )\n",
    "\n",
    "        # Extract layer information\n",
    "        elif \"|\" in line and \"layer_\" in line:\n",
    "            # Format: \"| layer_9/pre_attention_norm/scale   | (1152,)           | bfloat16 | 1,152       | 7.97      | 7.44    | ()       |\"\n",
    "            parts = [p.strip() for p in line.split(\"|\") if p.strip()]\n",
    "            if len(parts) >= 4:\n",
    "                layer_info = {\n",
    "                    \"name\": parts[0],\n",
    "                    \"shape\": parts[1],\n",
    "                    \"dtype\": parts[2],\n",
    "                    \"params\": (\n",
    "                        int(parts[3].replace(\",\", \"\"))\n",
    "                        if parts[3].replace(\",\", \"\").isdigit()\n",
    "                        else 0\n",
    "                    ),\n",
    "                }\n",
    "                result[\"layers\"].append(layer_info)\n",
    "                result[\"parameter_count\"] += layer_info[\"params\"]\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def parse_element_spec(spec_text: str) -> dict:\n",
    "    \"\"\"Parse element spec text and extract key information.\"\"\"\n",
    "    if not spec_text or not isinstance(spec_text, str):\n",
    "        return {}\n",
    "\n",
    "    result = {\n",
    "        \"input_spec\": None,\n",
    "        \"output_spec\": None,\n",
    "        \"has_input\": False,\n",
    "        \"has_output\": False,\n",
    "    }\n",
    "\n",
    "    # Look for input/output specifications\n",
    "    if \"input\" in spec_text.lower():\n",
    "        result[\"has_input\"] = True\n",
    "        # Try to extract input shape\n",
    "        if \"(\" in spec_text and \")\" in spec_text:\n",
    "            start = spec_text.find(\"(\")\n",
    "            end = spec_text.find(\")\", start)\n",
    "            if start != -1 and end != -1:\n",
    "                result[\"input_spec\"] = spec_text[start : end + 1]\n",
    "\n",
    "    if \"output\" in spec_text.lower():\n",
    "        result[\"has_output\"] = True\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def print_formatted_kpi():\n",
    "    \"\"\"Print KPI values with parsed parameter and element spec information.\"\"\"\n",
    "    manager = TensorBoardDataManager(ignore_timing=True)\n",
    "\n",
    "    metadata = manager.get_metadata()\n",
    "    latest_values = manager.get_latest_values()\n",
    "\n",
    "    if not metadata and not latest_values:\n",
    "        print(\"‚è≥ Waiting for training data...\")\n",
    "        return\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"üìä KEY PERFORMANCE INDICATORS\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Display metadata with parsing\n",
    "    if metadata:\n",
    "        print(\"\\nüèóÔ∏è MODEL INFORMATION\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        if \"num_params\" in metadata and metadata[\"num_params\"]:\n",
    "            print(f\"Model Parameters: {metadata['num_params']:,}\")\n",
    "\n",
    "        if \"parameters\" in metadata and metadata[\"parameters\"]:\n",
    "            param_summary = parse_parameter_summary(str(metadata[\"parameters\"]))\n",
    "            if param_summary.get(\"total_params\"):\n",
    "                print(f\"Total Parameters: {param_summary['total_params']:,}\")\n",
    "                print(\n",
    "                    f\"Total Memory: {param_summary['total_bytes']:,} bytes ({param_summary['total_bytes']/1024/1024:.1f} MB)\"\n",
    "                )\n",
    "                print(f\"Layers: {len(param_summary['layers'])}\")\n",
    "            else:\n",
    "                print(\"Parameters: Available (format not recognized)\")\n",
    "\n",
    "        if \"element_spec\" in metadata and metadata[\"element_spec\"]:\n",
    "            spec_info = parse_element_spec(str(metadata[\"element_spec\"]))\n",
    "            if spec_info.get(\"has_input\"):\n",
    "                print(f\"Input Spec: {spec_info.get('input_spec', 'Defined')}\")\n",
    "            else:\n",
    "                print(\"Element Spec: Available\")\n",
    "\n",
    "        if \"context_spec\" in metadata and metadata[\"context_spec\"]:\n",
    "            print(\"Context Spec: Available\")\n",
    "\n",
    "    \n",
    "# Run the function\n",
    "print_formatted_kpi()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemma",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
